ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
packages <- c("ggplot2", "plyr", "reshape2", "RColorBrewer", "scales", "grid", c("plyr","digest","ggplot2","colorspace","stringr","RColorBrewer","reshape2","zoo","proto","scales","car","dichromat","gtable","munsell","labeling","Hmisc","rJava","mvtnorm","bitops","rgl","foreign","XML","lattice","e1071","gtools","sp","gdata","Rcpp","MASS","Matrix","lmtest","survival","caTools","multcomp","RCurl","knitr","xtable","xts","rpart","evaluate","RODBC","tseries","DBI","nlme","lme4","reshape","sandwich","leaps","gplots","abind","randomForest","Rcmdr","coda","maps","igraph","formatR","maptools","RSQLite","psych","KernSmooth","rgdal","RcppArmadillo","effects","sem","vcd","XLConnect","markdown","timeSeries","timeDate","RJSONIO","cluster","scatterplot3d","nnet","fBasics","forecast","quantreg","foreach","chron","plotrix","matrixcalc","aplpack","strucchange","iterators","mgcv","kernlab","SparseM","tree","robustbase","vegan","devtools","latticeExtra","modeltools","xlsx","slam","TTR","quantmod","relimp","akima","memoise"))
ipak(packages)
install.packages("rstanarm")
install.packages("tidyverse")
install.packages("dplyr")
install.packages("data.table")
install.packages("lubridate")
install.packages("plotly")
install.packages("ggvis")
install.packages("mlr")
install.packages("xgboost")
install.packages("caret")
install.packages("gbm")
install.packages("prophet")
install.packages("randomForest")
library(rpart)
library(rpart.plot)
churnTrain<-churnTrain[-1]
churnTest<-churnTest[-1]
Churn_model<-lm(churn~.,data=churnTrain)
Churn_rpart <- rpart(Churn_model, method="class", data=churnTrain)
summary(Churn_rpart)
rpart.plot(Churn_rpart, type = 2,main="Classification Tree for Universal Bank")
library(caret)
actual <- churnTest$churn
predicted <- predict(Churn_rpart, churnTest, type="class")
results.matrix <- confusionMatrix(predicted, actual)
results.matrix
Churn_Calls <- read.csv("~/OneDrive - Rockhurst University/Courses/Data Mining/Week 2/Churn_Calls.csv")
View(Churn_Calls)
Churn_Calls <- read.csv("~/OneDrive - Rockhurst University/Courses/Data Mining/Week 2/Churn_Calls.csv")
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
churnTrain<-churnTrain[-1]
churnTest<-churnTest[-1]
Churn_model<-lm(churn~.,data=churnTrain)
Churn_rpart <- rpart(Churn_model, method="class", data=churnTrain)
summary(Churn_rpart)
rpart.plot(Churn_rpart, type = 2,main="Classification Tree for Universal Bank")
library(C50)
data(churn)
data(churnTrain)
data(churnTrain)
names(churnTest)
names(churnTrain)
library(rpart)
library(rpart.plot)
churnTrain<-churnTrain[-1]
churnTest<-churnTest[-1]
Churn_model<-lm(churn~.,data=churnTrain)
Churn_rpart <- rpart(Churn_model, method="class", data=churnTrain)
summary(Churn_rpart)
rpart.plot(Churn_rpart, type = 2,main="Classification Tree for Universal Bank")
install.packages("C5")
intrain<- createDataPartition(Churn_Calls$churn, p=0.7,list=FALSE)
set.seed(2017)
churnTest<- Churn_Calls[intrain,]
churnTrain<- Churn_Calls[-intrain,]
library(rpart)
library(rpart.plot)
churnTrain<-churnTrain[-1]
churnTest<-churnTest[-1]
Churn_model<-lm(churn~.,data=churnTrain)
Churn_rpart <- rpart(Churn_model, method="class", data=churnTrain)
summary(Churn_rpart)
rpart.plot(Churn_rpart, type = 2,main="Classification Tree for Universal Bank")
library(caret)
actual <- churnTest$churn
predicted <- predict(Churn_rpart, churnTest, type="class")
results.matrix <- confusionMatrix(predicted, actual)
results.matrix
rpart.plot(Churn_rpart, type = 3,main="Classification Tree for Universal Bank")
rpart.plot(Churn_rpart, type = 1,main="Classification Tree for Universal Bank")
rpart.plot(Churn_rpart, type = 4,main="Classification Tree for Universal Bank")
rpart.plot(Churn_rpart, type = 4,main="Classification Tree for Universal Bank")
rpart.plot(Churn_rpart, type = 0,main="Classification Tree for Universal Bank")
rpart.plot(Churn_rpart, type = 1,main="Classification Tree for Universal Bank")
rpart.plot(Churn_rpart, type = 3,main="Classification Tree for Universal Bank")
rpart.plot(Churn_rpart, type = 2,main="Classification Tree for Universal Bank")
results.matrix
Churn_Calls <- read.csv("~/OneDrive - Rockhurst University/Courses/Data Mining/Week 2/Churn_Calls.csv")
intrain<- createDataPartition(Churn_Calls$churn, p=0.7,list=FALSE)
set.seed(2017)
churnTest<- Churn_Calls[intrain,]
churnTrain<- Churn_Calls[-intrain,]
#library(C50)
#data(churn)
#data(churnTrain)
#data(churnTrain)
names(churnTest)
names(churnTrain)
library(rpart)
library(rpart.plot)
churnTrain<-churnTrain[-1]
churnTest<-churnTest[-1]
Churn_model<-lm(churn~.,data=churnTrain)
Churn_rpart <- rpart(Churn_model, method="class", data=churnTrain)
summary(Churn_rpart)
rpart.plot(Churn_rpart, type = 2,main="Classification Tree for Universal Bank")
library(caret)
actual <- churnTest$churn
predicted <- predict(Churn_rpart, churnTest, type="class")
results.matrix <- confusionMatrix(predicted, actual)
results.matrix
library(datasets)
data(AirPassengers)
AP <- AirPassengers
AP
#Attributes
class(AP)
length(AP)
start(AP)
end(AP)
summary(AP)
install.packages("fma")
install.packages(c("coda", "digest", "dplyr", "evaluate", "fansi", "foreign", "magick", "MASS", "mime", "OpenMx", "pkgbuild", "pkgload", "plotrix", "ps", "R6", "raster", "rgdal", "rstan", "rticles", "spatstat.data", "StanHeaders", "survey", "survival", "sysfonts", "testthat", "tidyselect"))
install.packages("gtrendsR")
library("gtrendsR")
#define the keywords
keywords=c("Paris","New York","Barcelona")
#set the geographic area: DE = Germany
country=c('DE')
#set the time window
time=("2010-01-01 2018-08-27")
#set channels
channel='web'
trends = gtrends(keywords, gprop =channel,geo=country, time = time )
#select only interst over time
time_trend=trends$interest_over_time
head(time_trend)
library(ggplot2)
plot<-ggplot(data=time_trend, aes(x=date, y=hits,group=keyword,col=keyword))+
geom_line()+xlab('Time')+ylab('Relative Interest')+ theme_bw()+
theme(legend.title = element_blank(),legend.position="bottom",legend.text=element_text(size=12))+ggtitle("Google Search Volume")
plot
View(time_trend)
time_trend2=time_trend[time_trend$hits<45,]
plot<-ggplot(data=time_trend2, aes(x=date, y=hits,group=keyword,col=keyword))+
geom_line()+xlab('Time')+ylab('Relative Interest')+ theme_bw()+
theme(legend.title = element_blank(),legend.position="bottom",legend.text=element_text(size=12))+ggtitle("Google Search Volume")
plot
install.packages("stargazer")
# loading needed libraries
library(ggstatsplot)
# for reproducibility
set.seed(123)
# plot
ggstatsplot::ggbetweenstats(
data = datasets::iris,
x = Species,
y = Sepal.Length,
messages = FALSE
) +                                               # further modification outside of ggstatsplot
ggplot2::coord_cartesian(ylim = c(3, 8)) +
ggplot2::scale_y_continuous(breaks = seq(3, 8, by = 1))
install.packages("ggstatsplot")
# loading needed libraries
library(ggstatsplot)
# for reproducibility
set.seed(123)
# plot
ggstatsplot::ggbetweenstats(
data = datasets::iris,
x = Species,
y = Sepal.Length,
messages = FALSE
) +                                               # further modification outside of ggstatsplot
ggplot2::coord_cartesian(ylim = c(3, 8)) +
ggplot2::scale_y_continuous(breaks = seq(3, 8, by = 1))
install.packages("broom.mixed")
# loading needed libraries
library(ggstatsplot)
# for reproducibility
set.seed(123)
# plot
ggstatsplot::ggbetweenstats(
data = datasets::iris,
x = Species,
y = Sepal.Length,
messages = FALSE
) +                                               # further modification outside of ggstatsplot
ggplot2::coord_cartesian(ylim = c(3, 8)) +
ggplot2::scale_y_continuous(breaks = seq(3, 8, by = 1))
library(ggplot2)
# for reproducibility
set.seed(123)
# let's leave out one of the factor levels and see if instead of anova, a t-test will be run
iris2 <- dplyr::filter(.data = datasets::iris, Species != "setosa")
# let's change the levels of our factors, a common routine in data analysis
# pipeline, to see if this function respects the new factor levels
iris2$Species <-
base::factor(x = iris2$Species,
levels = c("virginica" , "versicolor"))
# plot
ggstatsplot::ggbetweenstats(
data = iris2,
x = Species,
y = Sepal.Length,
notch = TRUE,                                   # show notched box plot
mean.plotting = TRUE,                           # whether mean for each group is to be displayed
mean.ci = TRUE,                                 # whether to display confidence interval for means
mean.label.size = 2.5,                          # size of the label for mean
type = "p",                                     # which type of test is to be run
bf.message = TRUE,                              # add a message with bayes factor in favor of the null
k = 2,                                          # number of decimal places for statistical results
outlier.tagging = TRUE,                         # whether outliers need to be tagged
outlier.label = Sepal.Width,                    # variable to be used for the outlier tag
outlier.label.color = "darkgreen",              # changing the color for the text label
xlab = "Type of Species",                       # label for the x-axis variable
ylab = "Attribute: Sepal Length",               # label for the y-axis variable
title = "Dataset: Iris flower data set",        # title text for the plot
ggtheme = ggthemes::theme_fivethirtyeight(),    # choosing a different theme
ggstatsplot.layer = FALSE,                      # turn off ggstatsplot theme layer
package = "wesanderson",                        # package from which color palette is to be taken
palette = "Darjeeling1",                        # choosing a different color palette
messages = FALSE
)
install.packages("broom.mixed")
install.packages(c("covr", "devtools", "rgdal", "rgeos", "rstanarm", "sf"))
Yes
install.packages(c("covr", "devtools", "rgdal", "rgeos", "rstanarm", "sf"))
install.packages(c("covr", "devtools", "rgdal", "rgeos", "rstanarm", "sf"))
install.packages(c("rlang", "rstanarm", "sf"))
library(rstanarm)
op <- options(contrasts = c("contr.helmert", "contr.poly"))
stan_aov(yield ~ block + N*P*K, data = npk,
prior = R2(0.5), seed = 12345)
options(op)
(fit <- stan_lm(mpg ~ wt + qsec + am, data = mtcars, prior = R2(0.75),
# the next line is only to make the example go fast enough
chains = 1, iter = 500, seed = 12345))
plot(fit, prob = 0.8)
plot(fit, "hist", pars = c("wt", "am", "qsec", "sigma"),
transformations = list(sigma = "log"))
library(datasets)
data(AirPassengers)
AP <- AirPassengers
AP
#Attributes
class(AP)
length(AP)
start(AP)
end(AP)
summary(AP)
# The base "plot" command
plot(AP, main = "Plot command")
plot.ts(AP, main = "Plot.ts command")
library(forecast)
Mean <- meanf(AP, h=48)
class(Mean)
head(Mean)
Naive <- naive(AP, h=48)
class(Naive)
head(Naive)
Seasonal <- snaive(AP, h=48)
class(Seasonal)
Seasonal
Drift <- rwf(AP, drift=TRUE, h=48)
class(Drift)
head(Drift)
library(ggplot2)
autoplot(Mean)
plot(Mean)
plot(Naive)
plot(Seasonal)
plot(Drift)
library(forecast)
library(ggplot2)
# ETS forecasts
USAccDeaths %>%
ets %>%
forecast %>%
autoplot
# Automatic ARIMA forecasts
WWWusage %>%
auto.arima %>%
forecast(h=20) %>%
autoplot
# ARFIMA forecasts
library(fracdiff)
x <- fracdiff.sim( 100, ma=-.4, d=.3)$series
arfima(x) %>%
forecast(h=30) %>%
autoplot
# Forecasting with STL
USAccDeaths %>%
stlm(modelfunction=ar) %>%
forecast(h=36) %>%
autoplot
library(tseries)
library(zoo)
#import dataset into a dataframe
setwd("/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/Week 1")
library(tseries)
library(zoo)
#import dataset into a dataframe
#setwd("/gartland/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/Week 1")
cisco = read.table('Data/cisco_00-10.csv', header=T, sep=',')
library(tseries)
library(zoo)
#import dataset into a dataframe
setwd("/gartland/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/Week 1")
setwd("/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/TimeSeries/time_series/Week 1")
setwd("/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/TimeSeries/time_series/Week 1")
setwd("/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/Week 1")
setwd("/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/time_series/Week 1")
library(tseries)
library(zoo)
#import dataset into a dataframe
setwd("/gartland/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/Week 1")
library(tseries)
library(zoo)
#import dataset into a dataframe
#setwd("/gartland/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/Week 1")
cisco = read.table('Data/cisco_00-10.csv', header=T, sep=',')
setwd("/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/time_series/Week 1")
library(tseries)
library(zoo)
#import dataset into a dataframe
#setwd("/gartland/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/Week 1")
cisco = read.table('Data/cisco_00-10.csv', header=T, sep=',')
library(tseries)
library(zoo)
#import dataset into a dataframe
setwd("/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/time_series/Week 1")
cisco = read.table('Data/cisco_00-10.csv', header=T, sep=',')
# create time series for cisco prices
ciscots = zoo(cisco$Price, as.Date(as.character(cisco$Date), format = "%m/%d/%y"))
#To retrieve only dates use
head(time(ciscots))
# Retrieve start date
start(ciscots)
# Retrieve End date
end(ciscots)
library(tseries)
library(zoo)
#import dataset into a dataframe
setwd("/Users/mylesgartland/OneDrive - Rockhurst University/Courses/Forecasting/Time Series/time_series/time_series/Week 1")
cisco = read.table('Data/cisco_00-10.csv', header=T, sep=',')
# create time series for cisco prices
ciscots = zoo(cisco$Price, as.Date(as.character(cisco$Date), format = "%m/%d/%y"))
#To retrieve only dates use
head(time(ciscots))
# Retrieve start date
start(ciscots)
# Retrieve End date
end(ciscots)
# sort data in chronological order
# set variable Date as time/date variable
cisco$Date=as.Date(as.character(cisco$Date), format = "%m/%d/%y")
cisco=cisco[order(cisco$Date),]
# sort data in chronological order
# set variable Date as time/date variable
cisco$Date=as.Date(as.character(cisco$Date), format = "%m/%d/%y")
cisco=cisco[order(cisco$Date),]
# sort data in chronological order
# set variable Date as time/date variable
cisco$Date=as.Date(as.character(cisco$Date), format = "%m/%d/%y")
cisco=cisco[order(cisco$Date),]
# sort data in chronological order
# set variable Date as time/date variable
cisco$Date=as.Date(as.character(cisco$Date), format = "%m/%d/%y")
cisco=cisco[order(cisco$Date),]
cisco
# create lagged series using function lag(tsobject, k==1);
pricelag = lag(ciscots, k=-1);
head(pricelag)
# diff = p_t - p_(t-1);
pricedif = diff(ciscots);
#compute simple returns ret = (p_t-p_(t-1))/p_(t-1)
ret=(ciscots-pricelag)/pricelag
# sort data in chronological order
# set variable Date as time/date variable
cisco$Date=as.Date(as.character(cisco$Date), format = "%m/%d/%y")
cisco=cisco[order(cisco$Date),]
# sort data in chronological order
# set variable Date as time/date variable
cisco$Date=as.Date(as.character(cisco$Date), format = "%m/%d/%y")
cisco=cisco[order(cisco$Date),]
cisco
# create lagged series using function lag(tsobject, k==1);
pricelag = lag(ciscots, k=-1);
head(pricelag)
# diff = p_t - p_(t-1);
pricedif = diff(ciscots);
#compute simple returns ret = (p_t-p_(t-1))/p_(t-1)
ret=(ciscots-pricelag)/pricelag
#Example of data analysis for cisco dataset
#DEFINE LOG RETURNS
#rts is a time series object since it is created from a TS object
rts = diff(log(ciscots))
#to retrieve numerical values from time series use coredata()
# rt is a numerical vector (no date information)
rt=coredata(rts)
#print first 6 values
head(rt)
library(fBasics)
# COMPUTE SUMMARY STATISTICS
basicStats(rt)
# CREATE HISTOGRAM
# OPTIONAL creates 2 by 2 display for 4 plots
# par(mfcol=c(2,2))
hist(rt, xlab="Cisco log returns", prob=TRUE, main="Histogram")
# add approximating normal density curve
xfit<-seq(min(rt),max(rt),length=40)
yfit<-dnorm(xfit,mean=mean(rt),sd=sd(rt))
lines(xfit, yfit, col="blue", lwd=2)
